{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python385jvsc74a57bd04f2d521a2815b2f711a75ded8da292dbb7e9bc64002662c00dc2549967a65c1f","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"preprocess_sakt.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"-ILlxvOlONg0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614246174712,"user_tz":-60,"elapsed":1591,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"94672b15-3b77-4ac3-a7e2-093b1244bd43"},"source":["import sys\n","import regex as re\n","import numpy as np\n","import io\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import math\n","import h5py\n","import time\n","import os\n","import datetime\n","import pickle\n","\n","LOCAL = True\n","BASE_DIR = '../'\n","\n","if not LOCAL:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    BASE_DIR = '/content/drive/My Drive/Colab Notebooks/thesis/'\n","\n","sys.path.append(BASE_DIR + 'lib')\n","sys.path.append(BASE_DIR + 'config')\n","from preprocessing import process_sakt, save_h5, transpose_list, process_one_feature, select_from_rows, progressBar\n","import dataset_parameters as params\n","\n","ONE_DAY = 86400\n","\n","# DATASET = 'akribian'\n","# DATASET = 'assistments_2012'\n","DATASET = 'junyi_academy'\n","# DATASET = 'ednet'\n","\n","INPUT_DIR = BASE_DIR + 'data/' + DATASET + '/raw/'\n","OUTPUT_DIR = BASE_DIR + 'data/' + DATASET + '/processed/'\n","# IN_FILE_NAME = 'rawdata.csv'\n","# IN_FILE_NAME = 'sorted.csv'\n","IN_FILE_NAME = 'transformed.csv'\n","FILE_NAME = 'processed.h5'\n","\n","DELETE_TIME_OUTLIERS = True\n","MAX_TIME = params.time_scale_dict[DATASET]\n","\n","TIME_STEPS = params.time_steps_dict[DATASET]\n","VALIDATION_RATIO = params.val_ratio_dict[DATASET]\n","TEST_RATIO = 0.01\n","SPLIT_SECTIONS = 20\n","\n","ROWS_PER_READ = 10000000\n","SHUFFLE = params.shuffle_dict[DATASET]\n","\n","# PADDING='pre'\n","PADDING='post'\n","\n","STRIDE = params.stride_dict[DATASET]"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxBy7HqMONg8"},"source":["## Load the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oc3oIvs0O9qg","executionInfo":{"status":"ok","timestamp":1614246175573,"user_tz":-60,"elapsed":2428,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"f847be99-0963-4c8d-aad6-67ce31ccdf9d"},"source":["# Columns to read from dataset\n","columns = params.columns_dict['generic']\n","group_column = columns[0]\n","encoding = params.encodings_dict[DATASET]\n","number_of_rows = -1\n","number_of_exercises = params.exercise_dict[DATASET]\n","number_of_ids = params.exercise_id_dict[DATASET]\n","time_scale = params.time_scale_dict[DATASET]\n","with open(OUTPUT_DIR + \"category_to_idx.pkl\", \"rb\") as f:\n","    category_to_index = pickle.load(f)\n","with open(OUTPUT_DIR + \"id_to_idx.pkl\", \"rb\") as f:\n","    id_to_index = pickle.load(f)"],"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{'PADDING': 0, '7f73q332BKPBXaixasa4EkUb+pF6VAsLxNIg4506JJs=': 1, 'R81Sqc8LAYj8amTPwFRvoPgbGpdaZoQLNX0hTg0DMB4=': 2, 'rzRcsBurW8jbUhivGAdZozPksRAZ5xM898ohJEBg93g=': 3, 'MfUX4BrIuFzJjm97tCQVisXbonyvtYtwCUJo6JpmoyU=': 4, 'jXSXg7CfDboPEXlnqJTGuQOb0VIgOXCpaU/Sl+/m3n0=': 5, '2YwsqJH0U7Zguyun1OaStQsIHbUoYvgJNK0QCGC5BQI=': 6, '5Np4fxxPeBgmNpeEOcXqarZIVsOEzZ1fSssL8cytQAc=': 7, '1EzKLzTq9Ax8/wlR9cJNrtthvk9lBi/SFdx/4L1PIaE=': 8, 'xYDz4OEv0xsri1IpmXlrgMLJ848rgySf+39xWpq4DBI=': 9, 'ICgke8JJv5eapCPwyj1aco8PEtoBkUbTZYIqxmYtqBk=': 10}\n"]}],"source":["print(category_to_index)"]},{"cell_type":"code","metadata":{"id":"i-ekVobzONg8","colab":{"base_uri":"https://localhost:8080/"},"tags":[],"executionInfo":{"status":"ok","timestamp":1614246206155,"user_tz":-60,"elapsed":32995,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"1e544899-c0e2-4b85-9b27-7ef346060cc6"},"source":["# Read head position for reading from file\n","read_index = 0\n","weighted_sum = 0\n","number_of_entries = 0\n","\n","try:\n","  os.remove(OUTPUT_DIR + FILE_NAME)\n","except:\n","  pass\n","\n","with h5py.File(OUTPUT_DIR + FILE_NAME, 'w') as hf:\n","  hf.create_dataset(\"dummy\", (1,))\n","\n","# Remember that DataFrames are immutable\n","while(True):  \n","  # Print progress\n","  if number_of_rows > 0:\n","    print(\"Reading %i/%i...\" % (math.ceil(read_index/ROWS_PER_READ) + 1, math.ceil(number_of_rows/ROWS_PER_READ) + 1))\n","  else:  \n","    print(\"Reading %i...\" % (math.ceil(read_index/ROWS_PER_READ) + 1))\n","\n","  start_time = time.time()\n","\n","  # Read rows\n","  data = pd.read_csv(INPUT_DIR + IN_FILE_NAME, encoding = encoding, nrows=ROWS_PER_READ, skiprows=[i for i in range(1, read_index)], usecols=columns)\n","\n","  print(\"Data read in %.2f seconds\" % (time.time() - start_time))\n","\n","  # If all the rows are read, break out of the loop\n","  if len(data) == 0:\n","      print(\"Out of usable data, breaking...\")\n","      break\n","\n","  if DELETE_TIME_OUTLIERS:\n","    data = data[data['elapsed_time'] < MAX_TIME]\n","\n","  print(\"90th percentile elapsed time for this selection of students: %.2f\" % np.percentile(data['elapsed_time'], 90))\n","  print(\"Mean elapsed time for this selection of students: %.2f\" % np.mean(data['elapsed_time']))\n","\n","  sys.stdout.write(\"Processing data... \")\n","\n","  print(\"Data processed.\")\n","  \n","  # Group rows\n","  sys.stdout.write(\"Grouping data... \")\n","  series = data.groupby([group_column]).agg({lambda x: list(x)})\n","  del data\n","  series_np = series.to_numpy()\n","  del series\n","  print(\"Done.\")\n","  sys.stdout.write(\"Transposing data... \")\n","  series_transposed = transpose_list(series_np)\n","  print(\"Done.\")\n","\n","  # Print average sequence length\n","  lengths = [len(x[0]) for x in series_np]\n","  avg_length = np.mean(lengths)\n","  weight = len(lengths)\n","  print(\"Average sequence length for this selection of students: %.2f\" % avg_length)\n","  weighted_sum += avg_length * weight\n","  number_of_entries += weight\n","  print(\"Max sequence length for this selection of students: %.2f\" % np.max(lengths))\n","  print(\"90th percentile sequence length for this selection of students: %.2f\" % np.percentile(lengths, 90))\n","  print(\"95th percentile sequence length for this selection of students: %.2f\" % np.percentile(lengths, 95))\n","  print(\"99th percentile sequence length for this selection of students: %.2f\" % np.percentile(lengths, 99))\n","\n","  # Process the rows\n","  sys.stdout.write(\"Running process_sakt... \")\n","  exs, ins, outs, ids = process_sakt(series_np, number_of_ids, TIME_STEPS, stride=STRIDE, padding=PADDING)\n","\n","  indices = np.arange(len(exs))\n","  if SHUFFLE:\n","    np.random.shuffle(indices)\n","\n","  print(\"Done.\")\n","  del series_np\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'exercises', exs[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append = read_index > 0)\n","  print(\"Saved exercises!\")\n","  del exs\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'exercise_ids', ids[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append = read_index > 0)\n","  print(\"Saved exercise ids!\")\n","  del ids\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'interactions', ins[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append = read_index > 0)\n","  print(\"Saved interactions!\")\n","  del ins\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'labels', outs[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append = read_index > 0)\n","  print(\"Saved labels!\")\n","  del outs\n","\n","  # Past label\n","  past_label = process_one_feature(series_transposed[1], TIME_STEPS, shift_data=True, dtype='float', stride=STRIDE, padding=PADDING)\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'past_labels', past_label[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0)\n","  print(\"Saved past labels!\")\n","  del past_label\n","  \n","  # Past elapsed time\n","  past_elapsed = process_one_feature(series_transposed[3], TIME_STEPS, shift_data=(DATASET != 'ednet'), dtype='float', stride=STRIDE, padding=PADDING)\n","  # past_elapsed = np.clip(past_elapsed, 0., 300.)\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'elapsed', past_elapsed[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0, dtype='float')\n","  print(\"Saved elapsed time!\")\n","  # Lag time\n","  timestamps = process_one_feature(series_transposed[4], TIME_STEPS, shift_data=False, dtype='float', stride=STRIDE, padding=PADDING)\n","  timestamps_shifted = process_one_feature(series_transposed[4], TIME_STEPS, shift_data=True, dtype='float', stride=STRIDE, padding=PADDING)\n","  timestamp_difference = timestamps - timestamps_shifted\n","  timestamp_difference = np.round(timestamp_difference - past_elapsed)\n","  del past_elapsed\n","  # Bound elapsed to [0, 300] and lag to [0, 1440] according to SAINT+\n","  timestamp_difference = np.clip(timestamp_difference, 0., ONE_DAY) / 60.\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'timestamps', timestamp_difference[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0, dtype='float')\n","  print(\"Saved timestamps!\")\n","  del timestamp_difference\n","  del timestamps_shifted\n","  del timestamps\n","\n","  # Current elapsed time\n","  current_elapsed = process_one_feature(series_transposed[3], TIME_STEPS, shift_data=(DATASET == 'ednet'), shift_forward=True, dtype='float', stride=STRIDE, padding=PADDING)\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'current_elapsed', current_elapsed[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0, dtype='float')\n","  print(\"Saved current elapsed time!\")\n","  del current_elapsed\n","\n","  # Current elapsed time\n","  current_elapsed_mean = process_one_feature(series_transposed[6], TIME_STEPS, shift_data=(DATASET == 'ednet'), shift_forward=True, dtype='float', stride=STRIDE, padding=PADDING)\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'current_elapsed_mean', current_elapsed_mean[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0, dtype='float')\n","  print(\"Saved current_elapsed_mean!\")\n","  del current_elapsed_mean\n","\n","  current_correctness_mean = process_one_feature(series_transposed[7], TIME_STEPS, shift_data=(DATASET == 'ednet'), shift_forward=True, dtype='float', stride=STRIDE, padding=PADDING)\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'current_correctness_mean', current_correctness_mean[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0, dtype='float')\n","  print(\"Saved current_correctness_mean!\")\n","  del current_correctness_mean\n","\n","\n","  past_elapsed_zscore = process_one_feature(series_transposed[5], TIME_STEPS, shift_data=(DATASET != 'ednet'), dtype='float', stride=STRIDE, padding=PADDING)\n","  save_h5(OUTPUT_DIR + FILE_NAME, 'past_elapsed_zscore', past_elapsed_zscore[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0, dtype='float')\n","  print(\"Saved past_elapsed_zscore!\")\n","  del past_elapsed_zscore\n","\n","  # current_elapsed_zscore = process_one_feature(series_transposed[5], TIME_STEPS, shift_data=(DATASET == 'ednet'), shift_forward=True, dtype='float', stride=STRIDE, padding=PADDING)\n","  # save_h5(OUTPUT_DIR + FILE_NAME, 'current_elapsed_zscore', current_elapsed_zscore[indices], validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0, dtype='float')\n","  # print(\"Saved current_elapsed_zscore!\")\n","  # del current_elapsed_zscore\n","\n","  # past_latency = process_one_feature(series_transposed[6], TIME_STEPS, shift_data=(DATASET != 'ednet'), dtype='float', stride=STRIDE)\n","  # save_h5(OUTPUT_DIR + FILE_NAME, 'past_latency', past_latency, validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0)\n","  # print(\"Saved past_latency!\")\n","  # del past_latency\n","\n","  # current_latency = process_one_feature(series_transposed[6], TIME_STEPS, shift_data=(DATASET == 'ednet'), shift_forward=True, dtype='float', stride=STRIDE)\n","  # save_h5(OUTPUT_DIR + FILE_NAME, 'current_latency', current_latency, validation_ratio=VALIDATION_RATIO, test_ratio=TEST_RATIO, split_sections=SPLIT_SECTIONS, append=read_index>0)\n","  # print(\"Saved current_latency!\")\n","  # del current_latency\n","\n","  del series_transposed\n","  print(\"Done.\")\n","\n","  # Delete unnecessary variables to free up ram\n","  read_index += ROWS_PER_READ\n","\n","  # test brake, uncomment to only use first n rows\n","  # break\n","\n","print(\"Dataset processing done!\")\n","print(\"Global average sequence length for this dataset: %.2f\" % (weighted_sum / number_of_entries))"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading 1...\n","Data read in 10.48 seconds\n","90th percentile elapsed time for this selection of students: 33.75\n","Mean elapsed time for this selection of students: 20.74\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 223.30\n","Max sequence length for this selection of students: 13354.00\n","90th percentile sequence length for this selection of students: 512.00\n","95th percentile sequence length for this selection of students: 1018.00\n","99th percentile sequence length for this selection of students: 2930.28\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 2...\n","Data read in 18.26 seconds\n","90th percentile elapsed time for this selection of students: 33.67\n","Mean elapsed time for this selection of students: 20.70\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 227.30\n","Max sequence length for this selection of students: 14636.00\n","90th percentile sequence length for this selection of students: 521.00\n","95th percentile sequence length for this selection of students: 1027.00\n","99th percentile sequence length for this selection of students: 3097.34\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 3...\n","Data read in 24.90 seconds\n","90th percentile elapsed time for this selection of students: 33.75\n","Mean elapsed time for this selection of students: 20.79\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 225.24\n","Max sequence length for this selection of students: 16459.00\n","90th percentile sequence length for this selection of students: 517.60\n","95th percentile sequence length for this selection of students: 1022.00\n","99th percentile sequence length for this selection of students: 3116.92\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 4...\n","Data read in 34.41 seconds\n","90th percentile elapsed time for this selection of students: 33.75\n","Mean elapsed time for this selection of students: 20.72\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 233.62\n","Max sequence length for this selection of students: 13798.00\n","90th percentile sequence length for this selection of students: 528.00\n","95th percentile sequence length for this selection of students: 1062.30\n","99th percentile sequence length for this selection of students: 3283.92\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 5...\n","Data read in 40.67 seconds\n","90th percentile elapsed time for this selection of students: 33.50\n","Mean elapsed time for this selection of students: 20.64\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 230.22\n","Max sequence length for this selection of students: 15628.00\n","90th percentile sequence length for this selection of students: 517.00\n","95th percentile sequence length for this selection of students: 1016.50\n","99th percentile sequence length for this selection of students: 3145.30\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 6...\n","Data read in 48.78 seconds\n","90th percentile elapsed time for this selection of students: 33.75\n","Mean elapsed time for this selection of students: 20.74\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 225.12\n","Max sequence length for this selection of students: 13593.00\n","90th percentile sequence length for this selection of students: 519.70\n","95th percentile sequence length for this selection of students: 1039.00\n","99th percentile sequence length for this selection of students: 3069.28\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 7...\n","Data read in 59.14 seconds\n","90th percentile elapsed time for this selection of students: 33.67\n","Mean elapsed time for this selection of students: 20.67\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 222.14\n","Max sequence length for this selection of students: 15732.00\n","90th percentile sequence length for this selection of students: 501.00\n","95th percentile sequence length for this selection of students: 988.45\n","99th percentile sequence length for this selection of students: 3123.49\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 8...\n","Data read in 65.20 seconds\n","90th percentile elapsed time for this selection of students: 34.00\n","Mean elapsed time for this selection of students: 20.66\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 224.09\n","Max sequence length for this selection of students: 14621.00\n","90th percentile sequence length for this selection of students: 510.00\n","95th percentile sequence length for this selection of students: 989.00\n","99th percentile sequence length for this selection of students: 2967.95\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 9...\n","Data read in 74.91 seconds\n","90th percentile elapsed time for this selection of students: 34.00\n","Mean elapsed time for this selection of students: 20.79\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 230.35\n","Max sequence length for this selection of students: 13385.00\n","90th percentile sequence length for this selection of students: 532.00\n","95th percentile sequence length for this selection of students: 1062.00\n","99th percentile sequence length for this selection of students: 3152.50\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 10...\n","Data read in 81.53 seconds\n","90th percentile elapsed time for this selection of students: 34.00\n","Mean elapsed time for this selection of students: 20.68\n","Processing data... Data processed.\n","Grouping data... Done.\n","Transposing data... Done.\n","Average sequence length for this selection of students: 226.03\n","Max sequence length for this selection of students: 15852.00\n","90th percentile sequence length for this selection of students: 518.00\n","95th percentile sequence length for this selection of students: 1027.50\n","99th percentile sequence length for this selection of students: 2967.50\n","Running process_sakt... Done.\n","Saved exercises!\n","Saved exercise ids!\n","Saved interactions!\n","Saved labels!\n","Saved past labels!\n","Saved elapsed time!\n","Saved timestamps!\n","Saved current elapsed time!\n","Saved current_elapsed_mean!\n","Saved current_correctness_mean!\n","Saved past_elapsed_zscore!\n","Done.\n","Reading 11...\n","Data read in 78.12 seconds\n","Out of usable data, breaking...\n","Dataset processing done!\n","Global average sequence length for this dataset: 226.70\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6wrGeIHKXmx","executionInfo":{"status":"ok","timestamp":1614246206157,"user_tz":-60,"elapsed":32985,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"87a1300f-b255-4a2a-9ddc-af82d691363d"},"source":["idx = 100\n","with h5py.File(OUTPUT_DIR + FILE_NAME, 'r') as hf:\n","  print(hf['exercises_train'].shape)\n","  print(hf['exercise_ids_train'].shape)\n","  print(hf['interactions_train'].shape)\n","  print(hf['labels_train'].shape)\n","  print(hf['elapsed_train'].shape)\n","  print(hf['timestamps_train'].shape)\n","\n","  print(hf['exercises_val'].shape)\n","  print(hf['exercise_ids_val'].shape)\n","  print(hf['interactions_val'].shape)\n","  print(hf['labels_val'].shape)\n","  print(hf['elapsed_val'].shape)\n","  print(hf['timestamps_val'].shape)\n","\n","  print(hf['exercises_test'].shape)\n","  print(hf['exercise_ids_test'].shape)\n","  print(hf['interactions_test'].shape)\n","  print(hf['labels_test'].shape)\n","  print(hf['elapsed_test'].shape)\n","  print(hf['timestamps_test'].shape)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["(385020, 1728)\n(385020, 1728)\n(385020, 1728)\n(385020, 1728)\n(385020, 1728)\n(385020, 1728)\n(20360, 1728)\n(20360, 1728)\n(20360, 1728)\n(20360, 1728)\n(20360, 1728)\n(20360, 1728)\n(3980, 1728)\n(3980, 1728)\n(3980, 1728)\n(3980, 1728)\n(3980, 1728)\n(3980, 1728)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHJdf8sqQEeH","executionInfo":{"status":"ok","timestamp":1614246208141,"user_tz":-60,"elapsed":34940,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"3d0f252e-f656-45b1-e5a6-8cef84b8cb22"},"source":["with h5py.File(OUTPUT_DIR + FILE_NAME, 'r') as hf:\n","  print(\"Unique exercise categories:\")\n","  print(len(list(set(hf['exercises_train'][:].flatten()))))\n","  print(len(list(set(hf['exercises_val'][:].flatten()))))\n","  # print(len(list(set(hf['exercises_test'][:].flatten()))))\n","  print(\"Max of exercise categories:\")\n","  print(np.max(hf['exercises_train'][:].flatten()))\n","  print(np.max(hf['exercises_val'][:].flatten()))\n","  # print(np.max(hf['exercises_test'][:].flatten()))\n","  train = hf['labels_train'][:].flatten()"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique exercise categories:\n","8\n","8\n","Max of exercise categories:\n","8\n","8\n"]}]},{"cell_type":"code","metadata":{"id":"6AjlAH7JRiHN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614246208143,"user_tz":-60,"elapsed":34932,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"fe78c887-3f77-4d47-d355-845547888450"},"source":["# Check for arrays consisting only of padding\n","print(\"Number of padding-only entries:\")\n","with h5py.File(OUTPUT_DIR + FILE_NAME, 'r') as hf:\n","  print(np.count_nonzero(np.all(hf['exercise_ids_train'][:] == 0, axis=1)))\n","  print(np.count_nonzero(np.all(hf['exercise_ids_val'][:] == 0, axis=1)))\n","  # print(np.count_nonzero(np.all(hf['exercise_ids_test'][:] == 0, axis=1)))"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of padding-only entries:\n","0\n","0\n"]}]},{"cell_type":"code","metadata":{"id":"MTadKw5AVbbj","executionInfo":{"status":"ok","timestamp":1614246208145,"user_tz":-60,"elapsed":34921,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"tags":[]},"source":["# Data example\n","index = 100\n","print(\"Sample test data:\")\n","with h5py.File(OUTPUT_DIR + FILE_NAME, 'r') as hf:\n","  print(\"\\nID, cat, interaction:\")\n","  print(hf['exercise_ids_val'][index])\n","  print(hf['exercises_val'][index])\n","  print(hf['interactions_val'][index])\n","  \n","  print(\"\\nLabel, past label:\")  \n","  print(hf['labels_val'][index])\n","  print(hf['past_labels_val'][index])\n","  \n","  print(\"\\nTimestamps:\")    \n","  print(hf['timestamps_val'][index])\n","\n","  print(\"\\nElapsed, past elapsed:\")    \n","  print(hf['current_elapsed_val'][index])\n","  print(hf['elapsed_val'][index])\n","\n","  print(\"\\nMean elapsed:\")    \n","  print(hf['current_elapsed_mean_val'][index])\n","  \n","  print(\"\\nMean correct:\")    \n","  print(hf['current_correctness_mean_val'][index])"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample test data:\n\nID, cat, interaction:\n[ 176 1279 2064 ...    0    0    0]\n[2 3 4 ... 0 0 0]\n[ 7877   176 14804 ...     0     0     0]\n\nLabel, past label:\n[0 1 0 ... 0 0 0]\n[0 0 1 ... 0 0 0]\n\nTimestamps:\n[0.11666667 0.06666667 0.51666667 ... 0.         0.         0.        ]\n\nElapsed, past elapsed:\n[22. 17. 17. ...  0.  0.  0.]\n[21. 22. 17. ...  0.  0.  0.]\n\nMean elapsed:\n[23.30789581 18.82637356 18.82637356 ...  0.          0.\n  0.        ]\n\nMean correct:\n[0.64918336 0.16311924 0.63365769 ... 0.         0.         0.        ]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}