{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python385jvsc74a57bd04f2d521a2815b2f711a75ded8da292dbb7e9bc64002662c00dc2549967a65c1f","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"preprocess_sakt.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"-ILlxvOlONg0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614246174712,"user_tz":-60,"elapsed":1591,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"94672b15-3b77-4ac3-a7e2-093b1244bd43"},"source":["import sys\n","import regex as re\n","import numpy as np\n","import io\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import math\n","import h5py\n","import time\n","import os\n","import datetime\n","import pickle\n","from scipy.stats import zscore\n","\n","LOCAL = True\n","BASE_DIR = '../'\n","\n","if not LOCAL:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    BASE_DIR = '/content/drive/My Drive/Colab Notebooks/thesis/'\n","\n","sys.path.append(BASE_DIR + 'lib')\n","sys.path.append(BASE_DIR + 'config')\n","from preprocessing import process_sakt, save_h5, transpose_list, process_one_feature, select_from_rows, progressBar\n","import dataset_parameters as params\n","\n","ONE_DAY = 86400\n","\n","# DATASET = 'akribian'\n","# DATASET = 'assistments_2012'\n","# DATASET = 'junyi_academy'\n","DATASET = 'ednet'\n","\n","INPUT_DIR = BASE_DIR + 'data/' + DATASET + '/raw/'\n","OUTPUT_DIR = BASE_DIR + 'data/' + DATASET + '/processed/sakt/'\n","# IN_FILE_NAME = 'rawdata.csv'\n","IN_FILE_NAME = 'sorted.csv'\n","FILE_NAME = 'transformed.csv'\n","\n","TIME_STEPS = params.time_steps_dict[DATASET]\n","VALIDATION_RATIO = 0.2\n","TEST_RATIO = 0.05\n","SPLIT_SECTIONS = 5\n","\n","Z_SCORE_CUTOFF = 0.33\n","\n","SHUFFLE = False\n","\n","STRIDE = params.stride_dict[DATASET]\n","CALCULATE_NUM_EXERCISES = False"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxBy7HqMONg8"},"source":["## Load the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oc3oIvs0O9qg","executionInfo":{"status":"ok","timestamp":1614246175573,"user_tz":-60,"elapsed":2428,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"f847be99-0963-4c8d-aad6-67ce31ccdf9d"},"source":["# Columns to read from dataset\n","columns = params.columns_dict[DATASET]\n","group_column = columns[0]\n","exercise_columns = columns[1]\n","correctness_columns = columns[2]\n","exercise_id_columns = columns[3]\n","response_time_columns = columns[4]\n","encoding = params.encodings_dict[DATASET]\n","\n","# Calculate how many tags and exercises we have\n","if CALCULATE_NUM_EXERCISES:\n","    cols = [exercise_columns, exercise_id_columns, response_time_columns]\n","\n","    print(\"Calculating number of unique exercises for dataset...\")\n","    start_time = time.time()\n","    print(\"Reading data...\")\n","    data = pd.read_csv(INPUT_DIR + IN_FILE_NAME, encoding = encoding, usecols=cols)\n","    data = data[cols]\n","\n","    if DATASET == 'assistments_2009':\n","        # Destroy rows lacking skill name\n","        data[\"skill_name\"].replace('', np.nan, inplace=True)\n","        data.dropna(subset=['skill_name'], inplace=True)\n","\n","    if DATASET == 'assistments_2012':\n","        # Destroy rows lacking skill name\n","        data[\"skill\"].replace('', np.nan, inplace=True)\n","        data.dropna(subset=['skill'], inplace=True)\n","\n","    if DATASET == 'ednet':\n","        # Filter the dataset so we only use the rows with content type 0 (question)\n","        data = data[data['content_type_id'] == 0]\n","        data.dropna(subset=[response_time_columns], inplace=True)\n","        metadata = pd.read_csv(INPUT_DIR + \"questions.csv\", usecols=['question_id', 'part'])\n","        metadata.rename(columns={\"question_id\": \"content_id\"} ,inplace=True)\n","        data = data.join(metadata, on='content_id', how='left', rsuffix='_dupe')    \n","        data = data[['part', exercise_id_columns, response_time_columns]]\n","\n","    if DATASET == 'junyi_academy':\n","        # Filter the dataset so we only use the rows with content type 0 (question)\n","        metadata = pd.read_csv(INPUT_DIR + \"Info_Content.csv\", encoding = encoding, usecols=['ucid', 'level2_id'])\n","        data = data.merge(metadata, on='ucid', how='left', suffixes=('', '_dupe'))    \n","        data.dropna(subset=['ucid'], inplace=True)\n","        data = data[['level2_id', exercise_id_columns, response_time_columns]]\n","\n","    print(\"Data read in %.2f seconds\" % (time.time() - start_time))\n","    print(\"Convering to numpy...\")\n","    series_np = data.to_numpy()\n","    series_transposed = transpose_list(series_np)\n","    series_np = series_transposed[0]\n","    series_np_id = series_transposed[1]\n","    series_times = series_transposed[2]\n","\n","    # Get categories\n","    print(\"Creating set...\")\n","    number_of_rows = len(series_np)\n","    unique_categories = ['PADDING'] + list(set(series_np))\n","    unique_ids = ['PADDING'] + list(set(series_np_id))\n","    number_of_exercises = len(unique_categories)\n","    number_of_ids = len(unique_ids)\n","    time_scale = np.percentile(series_times, 75)\n","    time_mean = np.mean(series_times)\n","    time_std = np.std(series_times)\n","    print(\"Number of exercise tags: %i\" % number_of_exercises)\n","    if (DATASET == 'ednet'):\n","        print(\"Max exercise id: \", np.max(data['content_id'][:]))\n","    print(\"Number of exercise ids: %i\" % number_of_ids)\n","    print(\"Number of rows: %i\" % number_of_rows)\n","    print(\"Average elapsed time: %.2f\" % time_mean)\n","    print(\"Std.dev elapsed time: %.2f\" % time_std)\n","    print(\"75th percentile elapsed time: %.2f\" % time_scale)\n","\n","    print(\"Deleting data\")\n","    del data\n","    del series_np\n","    print(\"Deleted!\")\n","\n","    print(\"Creating category dictionaries...\")    \n","\n","    category_to_index = {cat:idx for (idx, cat) in enumerate(unique_categories)}\n","    index_to_category = {v:k for (k,v) in category_to_index.items()}\n","    id_to_index = {cat:idx for (idx, cat) in enumerate(unique_ids)}\n","    index_to_id = {v:k for (k,v) in id_to_index.items()}\n","\n","    save_data = {\n","        number_of_exercises: number_of_exercises,\n","        number_of_ids: number_of_ids,\n","        time_scale: time_scale,\n","        time_mean: time_mean,\n","        time_std: time_std,\n","    }\n","\n","    with open(OUTPUT_DIR + \"category_to_idx.pkl\", \"wb\") as f:\n","        pickle.dump(category_to_index, f)\n","    with open(OUTPUT_DIR + \"id_to_idx.pkl\", \"wb\") as f:\n","        pickle.dump(id_to_index, f)\n","    with open(OUTPUT_DIR + \"save_data.pkl\", \"wb\") as f:\n","        pickle.dump(save_data, f)\n","\n","    \n","    print(\"Done!\")\n","\n","# Otherwise just load the values from the files\n","else:\n","    number_of_rows = -1\n","    number_of_exercises = params.exercise_dict[DATASET]\n","    number_of_ids = params.exercise_id_dict[DATASET]\n","    time_scale = params.time_scale_dict[DATASET]\n","    with open(OUTPUT_DIR + \"category_to_idx.pkl\", \"rb\") as f:\n","        category_to_index = pickle.load(f)\n","    with open(OUTPUT_DIR + \"id_to_idx.pkl\", \"rb\") as f:\n","        id_to_index = pickle.load(f)"],"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["epoch = datetime.datetime(1970,1,1)\n","def pr_akribian(row):\n","    # Turn category into integer\n","    row['LearningSequenceTitle'] = category_to_index[row['LearningSequenceTitle']]\n","    row['ExerciseTitle'] = id_to_index[row['ExerciseTitle']]\n","    # Use only first outcome\n","    row['SubmissionOutcomes'] = int(row['SubmissionOutcomes'][0])\n","    # Count timeout as wrong answer\n","    row['SubmissionOutcomes'] = min(row['SubmissionOutcomes'], 1)\n","    # Invert so 0 is wrong and 1 is right\n","    row['SubmissionOutcomes'] = 1 - row['SubmissionOutcomes']\n","    # \"11/23/2020 09:40:14 +00:00\"\n","    date = pd.to_datetime(row['FinishedOn'][:-7], format='%m/%d/%Y %H:%M:%S') - epoch\n","    row['FinishedOn'] = int(date.total_seconds())\n","    return row\n","\n","def pr_assistments_2009(row):\n","    # Turn category into integer\n","    row['skill_name'] = category_to_index[row['skill_name']]\n","    row['problem_id'] = id_to_index[row['problem_id']]\n","    return row\n","\n","def pr_assistments_2012(row):\n","    # Turn category into integer\n","    row['skill'] = category_to_index[row['skill']]\n","    row['problem_id'] = id_to_index[row['problem_id']]\n","    # 2012-10-22 18:44:03.013\n","    date = pd.to_datetime(row['end_time'], format='%Y-%m-%d %H:%M:%S.%f') - epoch\n","    row['end_time'] = int(date.total_seconds())\n","    return row\n","\n","def pr_junyi(row):\n","    # Turn category into integer\n","    row['ucid'] = id_to_index[row['ucid']]\n","    row['level2_id'] = category_to_index[row['level2_id']]\n","    # 2019-05-17 16:30:00 UTC    \n","    date = pd.to_datetime(row['timestamp_TW'][:-4], format='%Y-%m-%d %H:%M:%S') - epoch\n","    row['timestamp_TW'] = int(date.total_seconds())\n","    return row\n","\n","process_row_dict = {\n","    'akribian': pr_akribian,\n","    'assistments_2009': pr_assistments_2009,\n","    'assistments_2012': pr_assistments_2012,\n","    'junyi_academy': pr_junyi,\n","}"]},{"cell_type":"code","execution_count":22,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{'PADDING': 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7}\n"]}],"source":["print(category_to_index)"]},{"cell_type":"code","metadata":{"id":"i-ekVobzONg8","colab":{"base_uri":"https://localhost:8080/"},"tags":[],"executionInfo":{"status":"ok","timestamp":1614246206155,"user_tz":-60,"elapsed":32995,"user":{"displayName":"shamiranjaf","photoUrl":"","userId":"14651069388464291144"}},"outputId":"1e544899-c0e2-4b85-9b27-7ef346060cc6"},"source":["# Read rows\n","print(\"Reading data...\")\n","\n","start_time = time.time()\n","data = pd.read_csv(INPUT_DIR + IN_FILE_NAME, encoding = encoding, usecols=columns)\n","print(\"Data read in %.2f seconds\" % (time.time() - start_time))\n","\n","data = data[columns]\n","\n","sys.stdout.write(\"Processing data... \")\n","\n","# Filter bad data from dataset (depends entirely on the dataset used)\n","if DATASET == 'ednet':\n","  # Filter the dataset so we only use the rows with content type 0 (question)\n","  data = data[data['content_type_id'] == 0]\n","  data.dropna(subset=[response_time_columns], inplace=True)\n","  metadata = pd.read_csv(INPUT_DIR + \"questions.csv\", encoding = encoding, usecols=['question_id', 'part'])\n","  metadata.rename(columns={\"question_id\": \"content_id\"}, inplace=True)\n","  data = data.join(metadata, on='content_id', how='left', rsuffix='_dupe')\n","  data.dropna(subset=['content_id'], inplace=True)\n","  data = data[['user_id', 'part' ,'answered_correctly', 'content_id', 'prior_question_elapsed_time', 'timestamp']]\n","  data[['timestamp']] = data[['timestamp']] / 1000.\n","  data[['content_id']] = data[['content_id']] + 1\n","  data[['part']] = data[['part']] + 1\n","\n","if DATASET == 'junyi_academy':\n","  # Filter the dataset so we only use the rows with content type 0 (question)\n","  metadata = pd.read_csv(INPUT_DIR + \"Info_Content.csv\", encoding = encoding, usecols=['ucid', 'level2_id'])\n","  data = data.merge(metadata, on='ucid', how='left', suffixes=('', '_dupe'))\n","  data.dropna(subset=['ucid'], inplace=True)\n","  data = data[['uuid', 'level2_id' ,'is_correct', 'ucid', 'total_sec_taken', 'timestamp_TW']]\n","\n","if DATASET == 'assistments_2009':\n","  # Destroy rows lacking skill name\n","  orig_len = len(data)\n","  data[\"skill_name\"].replace('', np.nan, inplace=True)\n","  data.dropna(subset=['skill_name'], inplace=True)  \n","  print(\"Filtered %i rows\" % (orig_len - len(data)))\n","\n","if DATASET == 'assistments_2012':\n","  # Destroy rows lacking skill name\n","  orig_len = len(data)\n","  data[\"skill\"].replace('', np.nan, inplace=True)\n","  data.dropna(subset=['skill'], inplace=True)\n","  print(\"Filtered %i rows\" % (orig_len - len(data)))\n","\n","\n","print(\"Data processed.\")\n","\n","# Apply row-wise transformation, if any is defined, otherwise don't\n","if DATASET in process_row_dict.keys():\n","  process_row = process_row_dict[DATASET]\n","  print(\"Applying row-wise transformation...\")\n","  data = data.apply(process_row, axis=1)\n","\n","print(\"Done.\")"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading data...\n","Data read in 183.94 seconds\n","Processing data... Data processed.\n","Done.\n"]}]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Applying final adjustments...\n","Done.\n"]}],"source":["# 4 is elapsed time, 5 is timestamp\n","print(\"Applying final adjustments...\")\n","\n","if DATASET == \"assistments_2012\" or DATASET == \"ednet\":\n","    data.iloc[:,4] = data.iloc[:,4] / 1000.\n","\n","print(\"Done.\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating z-score for elapsed time...\n","C:\\Users\\Sepehr\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:2500: RuntimeWarning: invalid value encountered in true_divide\n","  return (a - mns) / sstd\n","Calculating mean for elapsed time...\n","Calculating mean for correctness...\n","Dataset processing done!\n"]}],"source":["mapping = {\n","  data.columns[0]: 'user_id',\n","  data.columns[1]: 'category',\n","  data.columns[2]: 'correctness',\n","  data.columns[3]: 'exercise_id',\n","  data.columns[4]: 'elapsed_time',\n","  data.columns[5]: 'timestamp',\n","}\n","\n","data = data.rename(columns = mapping)\n","\n","print(\"Calculating z-score for elapsed time...\")\n","data['elapsed_zscore'] = data.groupby(['exercise_id']).elapsed_time.transform(lambda x: zscore(x)).fillna(0)\n","print(\"Calculating mean for elapsed time...\")\n","data['elapsed_mean'] = data.groupby(['exercise_id']).elapsed_time.transform(lambda x: np.mean(x)).fillna(0)\n","print(\"Calculating mean for correctness...\")\n","data['correctness_mean'] = data.groupby(['exercise_id']).correctness.transform(lambda x: np.mean(x)).fillna(0)\n","\n","print(\"Dataset processing done!\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              user_id  category  correctness  exercise_id  elapsed_time  \\\n","1                 115         6            1         5717          37.0   \n","2                 115         2            1          129          55.0   \n","3                 115         2            1         7861          19.0   \n","4                 115         2            1         7923          11.0   \n","5                 115         2            1          157           5.0   \n","...               ...       ...          ...          ...           ...   \n","101230327  2147482888         6            1         3587          18.0   \n","101230328  2147482888         6            1         6342          14.0   \n","101230329  2147482888         6            1         4213          14.0   \n","101230330  2147482888         6            0         6344          22.0   \n","101230331  2147482888         6            1         7996          29.0   \n","\n","            timestamp  elapsed_zscore  elapsed_mean  correctness_mean  \n","1              56.943        0.836764     21.724882          0.735190  \n","2             118.363        1.874930     23.471403          0.984092  \n","3             131.167       -0.045952     19.462747          0.954813  \n","4             137.965       -1.208912     20.110520          0.953215  \n","5             157.063       -1.207056     18.810512          0.931887  \n","...               ...             ...           ...               ...  \n","101230327  428564.420       -0.306540     24.781216          0.741063  \n","101230328  428585.000       -0.510085     24.827936          0.528547  \n","101230329  428613.475       -0.534798     25.972512          0.616541  \n","101230330  428649.406       -0.127090     24.741762          0.665904  \n","101230331  428692.118        0.133342     25.887866          0.657558  \n","\n","[98878794 rows x 9 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>category</th>\n      <th>correctness</th>\n      <th>exercise_id</th>\n      <th>elapsed_time</th>\n      <th>timestamp</th>\n      <th>elapsed_zscore</th>\n      <th>elapsed_mean</th>\n      <th>correctness_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>115</td>\n      <td>6</td>\n      <td>1</td>\n      <td>5717</td>\n      <td>37.0</td>\n      <td>56.943</td>\n      <td>0.836764</td>\n      <td>21.724882</td>\n      <td>0.735190</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115</td>\n      <td>2</td>\n      <td>1</td>\n      <td>129</td>\n      <td>55.0</td>\n      <td>118.363</td>\n      <td>1.874930</td>\n      <td>23.471403</td>\n      <td>0.984092</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115</td>\n      <td>2</td>\n      <td>1</td>\n      <td>7861</td>\n      <td>19.0</td>\n      <td>131.167</td>\n      <td>-0.045952</td>\n      <td>19.462747</td>\n      <td>0.954813</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115</td>\n      <td>2</td>\n      <td>1</td>\n      <td>7923</td>\n      <td>11.0</td>\n      <td>137.965</td>\n      <td>-1.208912</td>\n      <td>20.110520</td>\n      <td>0.953215</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>115</td>\n      <td>2</td>\n      <td>1</td>\n      <td>157</td>\n      <td>5.0</td>\n      <td>157.063</td>\n      <td>-1.207056</td>\n      <td>18.810512</td>\n      <td>0.931887</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>101230327</th>\n      <td>2147482888</td>\n      <td>6</td>\n      <td>1</td>\n      <td>3587</td>\n      <td>18.0</td>\n      <td>428564.420</td>\n      <td>-0.306540</td>\n      <td>24.781216</td>\n      <td>0.741063</td>\n    </tr>\n    <tr>\n      <th>101230328</th>\n      <td>2147482888</td>\n      <td>6</td>\n      <td>1</td>\n      <td>6342</td>\n      <td>14.0</td>\n      <td>428585.000</td>\n      <td>-0.510085</td>\n      <td>24.827936</td>\n      <td>0.528547</td>\n    </tr>\n    <tr>\n      <th>101230329</th>\n      <td>2147482888</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4213</td>\n      <td>14.0</td>\n      <td>428613.475</td>\n      <td>-0.534798</td>\n      <td>25.972512</td>\n      <td>0.616541</td>\n    </tr>\n    <tr>\n      <th>101230330</th>\n      <td>2147482888</td>\n      <td>6</td>\n      <td>0</td>\n      <td>6344</td>\n      <td>22.0</td>\n      <td>428649.406</td>\n      <td>-0.127090</td>\n      <td>24.741762</td>\n      <td>0.665904</td>\n    </tr>\n    <tr>\n      <th>101230331</th>\n      <td>2147482888</td>\n      <td>6</td>\n      <td>1</td>\n      <td>7996</td>\n      <td>29.0</td>\n      <td>428692.118</td>\n      <td>0.133342</td>\n      <td>25.887866</td>\n      <td>0.657558</td>\n    </tr>\n  </tbody>\n</table>\n<p>98878794 rows Ã— 9 columns</p>\n</div>"},"metadata":{},"execution_count":26}],"source":["data"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing data to csv...\n","Data written in 1168.41 seconds\n"]}],"source":["start_time = time.time()\n","print(\"Writing data to csv...\")\n","data.to_csv(INPUT_DIR + FILE_NAME)\n","print(\"Data written in %.2f seconds\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}